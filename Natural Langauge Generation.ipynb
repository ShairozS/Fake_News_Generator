{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Generation\n",
    "## Shairoz Sohail (ssohai3) - University of Illinois - CS410\n",
    "\n",
    "-----------\n",
    "\n",
    "The generation of human-like natural language has been the holy grail of a large body of artificial intelligence research in the past several decades. The fabeled 'Turing Test' for the capabilities of an AI agent was singularly based around the succesful generation of natural language. While there have been many critics of the Turing Test and of natural language generation as an AI benchmark, it has nonetheless proven to be a very challenging and uniquely human task. Language is full of many intricies and subtle cues that become huge pitfalls for automated systems, and the highly dynamic nature of human language makes it hard to reduce to fixed logical structures.\n",
    "\n",
    "Over the past several years there have been significant breakthroughs in natural language generation, with one of the largest ones centering around the recent 'deep learning' revolution. Deep learning is a term given to the methodology of building learning algorithms using highly intricate neural network models, often with many ('deep') layers and complex architectures. A specific type of neural network that provides a time dependency strucutre, a recurrent neural network, has proven to be a leading tool for natural language generation. Hidden Markov Models, a more mathematically grounded method that uses conditional transition probabilities between words, has been used for natural language generation for a long time. In this guide, we take an overview of the task of natural language generation, describe these two leading approaches, and provide practical examples and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability\n",
    "\n",
    "Assume you are planning to go outside over the weekend and play a game of baseball. You are one homerun away from setting your personal record, however you are concered about the weather affecting your chances of hititng a homerun. You notice everytime it's a windy day, you have only a 2% chance of hitting a homerun. Normally, 5% of the hits you make are homeruns. Looking at the weather forecasts over the month, it looks like there's normally a 20% chance of this weekend being windy. We can set up this situation with random variables as follows:\n",
    "\n",
    "Y = Hitting a home run\n",
    "\n",
    "X = Is it a windy day?\n",
    "\n",
    "P(Y) = 0.05\n",
    "\n",
    "P(X) = 0.15\n",
    "\n",
    "P(Y|X) = 0.02\n",
    "\n",
    "Notice that if your home run odds were unaffected by the weather, X and Y would be independent of one another, hence P(Y|X) would equal P(Y)P(X)=(0.05)(0.15)=0.75 != 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the example above, words in a sentence are not conditionally independent. Intuitively, if you see the word 'sunny' in a sentence, it is much more likely to see the word 'day' then the word 'buffalo'. Mathematically, we may state this as the following:\n",
    "\n",
    "P('day' | 'sunny') > P('buffalo' | 'sunny')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a text document, we may actually compute the probabilities shown above simply counting word occurances. Let's give it a try in Python using the NLTK (natural language toolkit) and Numpy (numerical computing) packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"the quick brown fox jumped over the lazy dog. Then the quick brown fox was run over by the quicker SUV\"\n",
    "\n",
    "def get_conditional_probabilities(text, target_word, return_succesors=False):\n",
    "    succesors = []\n",
    "    text = word_tokenize(text) # Splitting the text into bag of words\n",
    "    # Finding occurances of the target_word in our text\n",
    "    word_occurances = [i for i,x in enumerate(text) if x == target_word]\n",
    "    # Retrieving \n",
    "    succesors = [text[i+1] for i in word_occurances]\n",
    "    if return_succesors==True:\n",
    "        return(Counter(succesors))\n",
    "    for succesor in set(succesors):\n",
    "        print(\"P(\" + str(succesor) + \" | \" + target_word + \") = \" + str((1/len(succesors))*succesors.count(succesor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(quick | the) = 0.5\n",
      "P(lazy | the) = 0.25\n",
      "P(quicker | the) = 0.25\n"
     ]
    }
   ],
   "source": [
    "get_conditional_probabilities(text = sample_text, target_word = \"the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chains\n",
    "\n",
    "Given a conditional probability, P(B|A), we may imagine it as 'transitioning' to a state B, given we're in state A. For example P(rain | cloudy) can be seen as the probability of transitioning into rainy weather, given that the weather is currently cloudy. We can go wild with this idea and have tons of states and the probabilities of transitioning between them. Such a network of states and transition probabilities is called a Markov Chain. \n",
    "\n",
    "\n",
    "If we let all the words in our document be states, using the fact that we can estimate the conditional transition probabilities as above, we can visualize documents as large markov chains. \n",
    "\n",
    "<image>\n",
    "    \n",
    "Since the transition probabilities are not known exactly (they are 'hidden'), we can estimate them using a large corpus of documents. Consider the following extract from the Wikipedia article on Mixed nuts. Normally, we'd build the Markov Chain from all the words in the text, but that would lead to a really long and messy output. We choose two common words ('from' and 'nuts') and use them to build our mock Markov Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text2 = '''\n",
    "In Japan, mixed nuts are the second most popular table nuts, behind sweet chestnuts;\n",
    "in the United States, they are second only to peanuts. Mixed nuts have also gained in popularity in the Argentinian \n",
    "market, which imported some $1.9 million in 1997, nearly half from the U.S. During the year 2002, \n",
    "U.S. companies sold $783 million of mixed nuts incorporating four or more varieties, mostly in canned form, \n",
    "representing hundreds of millions of pounds.The individual nuts that make up mixed nuts are harvested from \n",
    "all over the world. As a Dallas Fed publication supporting free trade puts it,\"In the average can of mixed nuts, \n",
    "you might find almonds from Italy, walnuts from China, Brazil nuts from Bolivia, cashews from India, pistachios from \n",
    "Turkey, hazelnuts from Canada—a true international assortment.\"Label on a jar representing eight countries\n",
    "This reality provides an incentive for nut salters to favor free trade for nuts, as opposed to nut farmers, \n",
    "who would generally support trade barriers. In fact, one historical argument for United States salters is \n",
    "that importing nuts can encourage domestic production, since mixed nuts provide a \"wagon\" on which \n",
    "everyone's sales ride. For example, cashews are not produced in North America, and it is necessary to \n",
    "import them because mixed nuts are essential to the sale of pecans, which are grown exclusively in North America\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "    if vocabulary is None:\n",
    "        vocabulary = word_tokenize(text)\n",
    "    text = word_tokenize(text)\n",
    "    for word_occurance in vocabulary:\n",
    "        succesors = []\n",
    "        # Finding occurances of the target_word in our text\n",
    "        word_occurances = [i for i,x in enumerate(text) if x == word_occurance]\n",
    "        # Retrieving \n",
    "        succesors = [text[i+1] for i in word_occurances]\n",
    "        for succesor in succesors:\n",
    "            print(word_occurance + '---' + str(round((1/len(succesors))*succesors.count(succesor),2)) + '-->' + str(succesor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nuts---0.25-->are\n",
      "nuts---0.25-->,\n",
      "nuts---0.08-->have\n",
      "nuts---0.08-->incorporating\n",
      "nuts---0.08-->that\n",
      "nuts---0.25-->are\n",
      "nuts---0.25-->,\n",
      "nuts---0.08-->from\n",
      "nuts---0.25-->,\n",
      "nuts---0.08-->can\n",
      "nuts---0.08-->provide\n",
      "nuts---0.25-->are\n",
      "from---0.12-->the\n",
      "from---0.12-->all\n",
      "from---0.12-->Italy\n",
      "from---0.12-->China\n",
      "from---0.12-->Bolivia\n",
      "from---0.12-->India\n",
      "from---0.12-->Turkey\n",
      "from---0.12-->Canada—a\n"
     ]
    }
   ],
   "source": [
    "build_markov_chain(sample_text2, vocabulary = ['nuts', 'from'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you can probably guess, traversing through a Markov Chain will generate a sequence of words that should look similar to some of the text used to generate the Markov Chain. If we feed our MC a large enough collection of text, we may even see novel ideas and passages emerge. What if we uses a sample of 10,000 song lyrics from the 55,000 song lyrics dataset to generate our Markov Chain? We can easily test this idea by getting an efficient implementation of a Markov Chain and feeding it the dataset. There are some text artifacts from data formatting, but overall it should do a pretty good job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    " class markov_langauge_model:\n",
    "    def __init__ (self, order=1, seed=101):\n",
    "        self.order = order\n",
    "        self.group_size = self.order + 1\n",
    "        self.text = None\n",
    "        self.graph = {}\n",
    "        return\n",
    "\n",
    "    def train(self, filename): #receive the text file\n",
    "        self.text = (filename).read().split()\n",
    "        #self.text = self.text + self.text[: self.order]\n",
    "        for i in range(0, len(self.text) - self.group_size):\n",
    "            key = tuple(self.text[i: i + self.order])\n",
    "            value = self.text[i + self.order]\n",
    "            if key in self.graph:\n",
    "                self.graph[key].append(value)\n",
    "            else :\n",
    "                self.graph[key] = [value]\n",
    "        return\n",
    "    \n",
    "    def inspect_graph(self):\n",
    "        print(self.graph.keys())\n",
    "\n",
    "    def generate (self,length):\n",
    "        random.seed(self.seed)\n",
    "        index = random.randint (0, len (self.text) - self.order)\n",
    "        result = self.text[index : index + self.order]\n",
    "        for i in range (length):\n",
    "            state = tuple (result [len (result) - self.order : ] )\n",
    "            next_word = random.choice (self.graph [state] )\n",
    "            result.append (next_word)\n",
    "        s=\" \".join (result [self.order : ] )\n",
    "        s = (filter(lambda x: x.isalpha(), word_tokenize(s)))\n",
    "        return(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'markov_langauge_model' object has no attribute 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-e6bacd832977>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarkov_langauge_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/User/Desktop/Text_Generation/song_lyrics.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-129-936aa9b1824c>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m        \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m        \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m        \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'markov_langauge_model' object has no attribute 'seed'"
     ]
    }
   ],
   "source": [
    "new_model = markov_langauge_model()\n",
    "new_model.train(filename=open('/Users/User/Desktop/Text_Generation/song_lyrics.txt','r'))\n",
    "new_model.generate(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Chain Order - Modeling longer ideas in text\n",
    "\n",
    "Wonderful! So far we've looked at conditional probabilities and used those to build a Markov Chain for our text. A natural question to ask at this point would be: why are we only looking at the prior word to choose the next word? Humans have a much longer memory span then that, we don't just form our sentences by thinking about the previous word we spoke. What if we used conditional probabilities for pairs, or triplets of words? this is the concept of order in a Markov Chain. The chain we build above is of order 1. While the math becomes more complicated, we can build Markov Chains of higher order to generate more natural sounding text. Long-term dependencies are automatically captured in a LSTM neural network, however let's try to max out the Markov Chain method as much as we can first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey there, Mr. Cold, Cold Heart, I know I could be But it\\'s just one of those days \" \"2835\",\"I just want to be with you everywhere Oh I, I want to be with you everywhere Oh I, I want to be with you Don\\'t give up on me I\\'ll meet you when my chores are through I don\\'t know how long I\\'ll be But I\\'m not gonna let you down Darling wait and see \\'Cause between now and then, till I see you again I\\'ll be loving you, love me If you get there before I do Don\\'t give up on me Don\\'t give up on me I\\'ll meet you when my chores are through I don\\'t know how long it\\'ll last But I never regret the past When I wake up tomorrow, I\\'ll be looking around for you I got no reason to be down at the station'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = markov_langauge_model(5)\n",
    "new_model.train(filename=open('/Users/User/Desktop/Text_Generation/song_lyrics.txt','r'))\n",
    "new_model.generate(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Finding the optimal order (we used 5 here) for human-like text is mostly a trial-and-error process, but even with this randomly selected order we got pretty awesome results. I for one, welcome our new robot overlords!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Some of the state-of-the-art results in machine learning have been accomplished with models known as artificial neural networks (ANNs). The very core ideas of these models is easy to state:\n",
    "\n",
    "\n",
    "We'd like to predict a variable Y, given a variable X. Just like the above examples, we'd like to know P(Y|X). One way to do this is to try to model the function that converts the input X into Y, that is try to model f such that f(X)=Y. Assume you are given the following dataset:\n",
    "\n",
    "    X        Y\n",
    "   ---      ---\n",
    "    8        80\n",
    "    5        50\n",
    "    4        40\n",
    "    6        60\n",
    "    7        70\n",
    "    7.5      75\n",
    "    9.2      92\n",
    "    12       95\n",
    "    13       85\n",
    "    9        90\n",
    "    \n",
    "\n",
    "Reading off the first few results, we might think it would be a good general rule to say Y = X*10, or f(x) = 10x. This method does pretty good, yielding Y values {80,50,40,60,70,75,92,120,130,90}. The errors from the actual predictions are {0,0,0,0,0,0,0,25,45,0}, for an average error of 25+45/10 = 7. Not bad.\n",
    "\n",
    "Let's plot our results\n",
    "\n",
    "![Image of Yaktocat](xy_plot.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it looks like our predictions are shooting off into infinity as X gets larger, while the actual Y seems bounded above by 100. While we'd need more data to make solid conclusions, it is not unlikely that this is data for hours of sleep vs. test score or something similar that has an upper bound. Since we can only get a straight line with the multiplication idea we had above, we need something a little more advanced to model the diminishing marginal returns of X. \n",
    "\n",
    "In essence, we need to introduce a function that produces a curved line instead of a straight one, that is to say a non-linear function. In a neural network, this is done simply by picking a non-linear function (such as x^2 or tanh(x)) and applying it after multiplying by the weight. This might yield a curve as follows\n",
    "\n",
    "![Image of Yaktocat](xy_plot2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know we can use two basic ideas to generate some pretty good predictions:\n",
    "  \n",
    "  1) Multiplying X by a number (call it W) = W*X\n",
    "  \n",
    "  2) Taking the result and passing it through a non-linear function P(WX)\n",
    "  \n",
    "Turns out, that's most of all we need! A neural network is simply a large connection of objects called neurons, each of which simply does the two steps above to it's input. Here's an example of a neural network with only a single neuron\n",
    "\n",
    "![single_neural_nn](neural_network_single.png)\n",
    "\n",
    "However, this is not a very powerful network! We should add more neurons! Since this will cause multiple Y's to be output, we need another neuron to aggregate the results.\n",
    "\n",
    "![multiple_neuron_nn](neural_network_dense.png)\n",
    "\n",
    "These networks decide their weights based on some pre-defined cost function, which captures the difference between the predicted values of Y and the actual values of Y. Since cost functions that are chosen are usually differentiable, we can nudge the weights in the right direction so that each time we decide new weights they make the prediction closer to the actual values of Y. This is known as gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Deep Learning\" is simply the idea of creating neural networks such as the one above, but with multiple layers. This makes it easier to model complex dependencies and 'build-up' complex concepts from simple ideas. However, what do we feed this network to do langauge generation? We could feed it the prior word we've seen to predict the next one, however this will suffer from the same problem that the Markov Chian of order 1 suffered from - limited memory. We need a neural network that can take in sequential input (such as a sentence) and remember long-term dependencies... \n",
    "\n",
    "enter the 'Recurrent Neural Network'\n",
    "\n",
    "*image courtesy of leonardo araujo santos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![recurrent_neural_network](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_6/recurrent.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an extremely simplified view, wiring the neural network back into itself allows it to keep track of some of the previous inputs it's been fed. This may immediately raise many questions. How far back in inputs can it keep track of? How does it integrate this knowledge into new predictions? Unfortunately, the answers to these questions are not very straightforward and depend on how the network is wired up. For now, we can simply imagine RNN's working very similar to regular ANN's but with the addition of this circular wiring to help remember dependencies.\n",
    "However, even this very intricate RNN has trouble remembering really long-term dependencies. A human is required to perform extensive tuning that depends on the type of dataset we're using and also takes a long time. A more intricate version of the RNN, known as a Long-Short-Term-Memory Recurrent Neural Network (or LSTM-RNN for short) takes care of this problem and is able to much more easily 'learn' the proper dependency structure in the text. \n",
    "\n",
    "We use a large sample of Nietzsche's (a prolific germal philosopher) essays to train the chracter-level LSTM-RNN from the Python Keras package. Note, this model takes several hours of training before it starts to produce barely coherent text, however the large amount of parameters allows it to more precisely learn the long-term dependencies and style in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        # Stop training when output text begins looking coherent\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating with seed: \"has the same origin, and follow the scen\"\n",
      "\n",
      "has the same origin, and follow the scend and and the strong to the presented to himself the proved and the most strong and also the soul, the standard themselves and interpreted the philosopher and standard and stand the power and and and the more the proved as the presented and the origination of the sense is and hadming the soul and something the standard of the soul, the solitary of the surprised the subtle and about the presented a\n"
     ]
    }
   ],
   "source": [
    "for diversity in [0.2]:\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        print()\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In review, we went over the ideas of conditional probability and Markov Chains to demonstrate how a document can be seen as a graph of states (words) and the probabilities of transitioning between them. Given these, we can simply traverse the graph to generate new documents. We also reviewed the ideas behind Recurrent Neural Networks, and showed how they can use 'memory' cells to model arbitrarily long dependencies in text and generate new text which capture larger themes and topics such as those found in philosophy essays. These methods are currentely considered state-of-the-art in natural language generation and are finding new applications everyday (espacially in the form of voice assistants or automated customer-service centers). The flexibility of deep learning models like LSTM-RNN and the exponentially growing amount of text data being created every day is sure to see even more advances and breakthroughs in the exciting area of Natural Language Generation, to the point where not only is the Turing Test passed but computer-generated natural langauge becomes a critical part of global communication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Markov Model of Natural Language, CS at Princeton\n",
    "http://www.cs.princeton.edu/courses/archive/spr05/cos126/assignments/markov.html\n",
    "\n",
    "- Language Modeling, Stanford CS Department\n",
    "https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf\n",
    "\n",
    "- Recurrent Neural Networks, Stanford CS Department\n",
    "http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf\n",
    "\n",
    "- Understanding LSTM Networks, Christopher Olah\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "- Poetry in Python: Using Markov Chains to Generate Texts, Omer Nevo\n",
    "http://il.pycon.org/2016/static/sessions/omer-nevo.pdf\n",
    "\n",
    "- Keras Neural Network Examples\n",
    "https://github.com/fchollet/keras/tree/master/examples\n",
    "\n",
    "- 55,000 song lyrics dataset, Kaggle+LyricsFreak\n",
    "https://www.kaggle.com/mousehead/songlyrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
